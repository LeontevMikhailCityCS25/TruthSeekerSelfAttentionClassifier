{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Dataset origin:** https://www.unb.ca/cic/datasets/truthseeker-2023.html\n",
    "\n",
    "*S. Dadkhah, X. Zhang, A. G. Weismann, A. Firouzi and A. A. Ghorbani, \"The Largest Social Media Ground-Truth Dataset for Real/Fake Content: TruthSeeker,\" in IEEE Transactions on Computational Social Systems, 99. 1-15, Oct. 2023.*\n",
    "\n",
    "Pytorch self-attention: https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"
   ],
   "id": "a884aadfc555abbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T12:45:16.774024Z",
     "start_time": "2025-10-15T12:45:12.082237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ],
   "id": "52574a5d7815c9a6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailleontev/PycharmProjects/Attestation/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:16:21.474171Z",
     "start_time": "2025-10-14T14:16:21.119690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PATH_TO_FILE =\"/Users/mikhailleontev/PycharmProjects/Attestation/TruthSeeker2023/Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(PATH_TO_FILE)\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "id": "e861b48c9f1f1e9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134198, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0      author                                          statement  \\\n",
       "0           0  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "1           1  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "2           2  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "3           3  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "4           4  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "\n",
       "   target  BinaryNumTarget                 manual_keywords  \\\n",
       "0    True              1.0  Americans, eviction moratorium   \n",
       "1    True              1.0  Americans, eviction moratorium   \n",
       "2    True              1.0  Americans, eviction moratorium   \n",
       "3    True              1.0  Americans, eviction moratorium   \n",
       "4    True              1.0  Americans, eviction moratorium   \n",
       "\n",
       "                                               tweet 5_label_majority_answer  \\\n",
       "0  @POTUS Biden Blunders - 6 Month Update\\n\\nInfl...            Mostly Agree   \n",
       "1  @S0SickRick @Stairmaster_ @6d6f636869 Not as m...             NO MAJORITY   \n",
       "2  THE SUPREME COURT is siding with super rich pr...                   Agree   \n",
       "3  @POTUS Biden Blunders\\n\\nBroken campaign promi...            Mostly Agree   \n",
       "4  @OhComfy I agree. The confluence of events rig...                   Agree   \n",
       "\n",
       "  3_label_majority_answer  \n",
       "0                   Agree  \n",
       "1                   Agree  \n",
       "2                   Agree  \n",
       "3                   Agree  \n",
       "4                   Agree  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>statement</th>\n",
       "      <th>target</th>\n",
       "      <th>BinaryNumTarget</th>\n",
       "      <th>manual_keywords</th>\n",
       "      <th>tweet</th>\n",
       "      <th>5_label_majority_answer</th>\n",
       "      <th>3_label_majority_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@POTUS Biden Blunders - 6 Month Update\\n\\nInfl...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@S0SickRick @Stairmaster_ @6d6f636869 Not as m...</td>\n",
       "      <td>NO MAJORITY</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>THE SUPREME COURT is siding with super rich pr...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@POTUS Biden Blunders\\n\\nBroken campaign promi...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@OhComfy I agree. The confluence of events rig...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:16:22.931398Z",
     "start_time": "2025-10-14T14:16:22.926860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tweets = df['tweet']\n",
    "tweets.head(10)"
   ],
   "id": "74f8c60f25090bfe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @POTUS Biden Blunders - 6 Month Update\\n\\nInfl...\n",
       "1    @S0SickRick @Stairmaster_ @6d6f636869 Not as m...\n",
       "2    THE SUPREME COURT is siding with super rich pr...\n",
       "3    @POTUS Biden Blunders\\n\\nBroken campaign promi...\n",
       "4    @OhComfy I agree. The confluence of events rig...\n",
       "5    I've said this before, but it really is incred...\n",
       "6    As many face backlogged rent payments, America...\n",
       "7    @Thomas1774Paine @JoeBiden\\n#DOJ@TheJusticeDep...\n",
       "8    @SocialismIsDone @TheeKHiveQueenB Its a win fo...\n",
       "9    @daysofarelives2 @Sen_JoeManchin There is not ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:16:24.474189Z",
     "start_time": "2025-10-14T14:16:24.466953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labes = df['BinaryNumTarget']\n",
    "print(labes.value_counts())\n",
    "labes.head(10)"
   ],
   "id": "2960e766b4772fbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryNumTarget\n",
      "1.0    68930\n",
      "0.0    65268\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "5    1.0\n",
       "6    1.0\n",
       "7    1.0\n",
       "8    1.0\n",
       "9    1.0\n",
       "Name: BinaryNumTarget, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:16:46.515930Z",
     "start_time": "2025-10-14T14:16:27.187598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_tweets = []\n",
    "i = 0\n",
    "for tweet in tweets:\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    tokenized_tweets.append(tokens)\n",
    "    i += 1\n",
    "    if i % 10000 == 0:\n",
    "        print(f'Tokenized {i} tweets')\n"
   ],
   "id": "ba20dc8f3df1fef2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 10000 tweets\n",
      "Tokenized 20000 tweets\n",
      "Tokenized 30000 tweets\n",
      "Tokenized 40000 tweets\n",
      "Tokenized 50000 tweets\n",
      "Tokenized 60000 tweets\n",
      "Tokenized 70000 tweets\n",
      "Tokenized 80000 tweets\n",
      "Tokenized 90000 tweets\n",
      "Tokenized 100000 tweets\n",
      "Tokenized 110000 tweets\n",
      "Tokenized 120000 tweets\n",
      "Tokenized 130000 tweets\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:16:48.267687Z",
     "start_time": "2025-10-14T14:16:48.228172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tweet_lengths = [len(tokens) for tokens in tokenized_tweets]\n",
    "print(f'Mean length of tokenized tweets: {np.mean(tweet_lengths)}')\n",
    "print(f'Median tweet length: {np.median(tweet_lengths)}')\n",
    "print(f'Max length of tokenized tweets: {max(tweet_lengths)}')\n",
    "print(f'Min length of tokenized tweets: {min(tweet_lengths)}')\n"
   ],
   "id": "b479a7c0cdd21eee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of tokenized tweets: 42.12054576074159\n",
      "Median tweet length: 44.0\n",
      "Max length of tokenized tweets: 174\n",
      "Min length of tokenized tweets: 1\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:16:50.730709Z",
     "start_time": "2025-10-14T14:16:50.316947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CAP_LENGTH = 50\n",
    "tweets_capped = [tokens[:CAP_LENGTH] for tokens in tokenized_tweets]"
   ],
   "id": "40970a511c571af8",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:16:53.138540Z",
     "start_time": "2025-10-14T14:16:53.135709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Word2Vec Hyper parameters\n",
    "VECTOR_SIZE = 64\n",
    "WINDOW = 5\n",
    "WORKERS = 4"
   ],
   "id": "774f50d7fbbe7d08",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:17:03.101108Z",
     "start_time": "2025-10-14T14:16:54.731197Z"
    }
   },
   "cell_type": "code",
   "source": "model_vec = Word2Vec(tweets_capped, vector_size=VECTOR_SIZE, window=WINDOW, min_count=1, workers=WORKERS)",
   "id": "edc1f8537f030ab9",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:17:10.656492Z",
     "start_time": "2025-10-14T14:17:05.274809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedded_tweets = []\n",
    "for tokens in tweets_capped:\n",
    "    tweet_vector = []\n",
    "    for token in tokens:\n",
    "        tweet_vector.append(model_vec.wv[token])\n",
    "    # Pad with zero vectors if tweet is shorter than CAP_LENGTH\n",
    "    while len(tweet_vector) < CAP_LENGTH:\n",
    "        tweet_vector.append(np.zeros(VECTOR_SIZE))\n",
    "    embedded_tweets.append(tweet_vector)\n",
    "embedded_tweets = np.array(embedded_tweets) # turn into numpy array\n",
    "print(embedded_tweets.shape)"
   ],
   "id": "1710f948ae4ef321",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134198, 50, 64)\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:17:15.040729Z",
     "start_time": "2025-10-14T14:17:13.838336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 20042004\n",
    "SHUFFLE = True\n",
    "train_x, test_x, train_y, test_y = train_test_split(embedded_tweets, labes, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=SHUFFLE)\n",
    "print(f'Train shape: {train_x.shape}, Test shape: {test_x.shape}')\n",
    "print(f'Train labels shape: {train_y.shape}, Test labels shape: {test_y.shape}')"
   ],
   "id": "1013acf510bf5557",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (107358, 50, 64), Test shape: (26840, 50, 64)\n",
      "Train labels shape: (107358,), Test labels shape: (26840,)\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:17:19.212438Z",
     "start_time": "2025-10-14T14:17:19.206333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define torch model\n",
    "# Hyperparameters\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "NUMBER_OF_HEADS = 4\n",
    "\n",
    "class TweeterClassifier (torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TweeterClassifier, self).__init__()\n",
    "        self.self_attention = torch.nn.MultiheadAttention(embed_dim=VECTOR_SIZE, num_heads=NUMBER_OF_HEADS)\n",
    "        self.fc1 = torch.nn.Linear(CAP_LENGTH * VECTOR_SIZE, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, CAP_LENGTH, VECTOR_SIZE)\n",
    "        x = x.permute(1, 0, 2)  # Change to (CAP_LENGTH, batch_size, VECTOR_SIZE)\n",
    "        attn_output, _ = self.self_attention(x, x, x)\n",
    "        attn_output = attn_output.permute(1, 0, 2)\n",
    "        # shape: (batch_size, CAP_LENGTH, VECTOR_SIZE)\n",
    "        attn_output = attn_output.reshape(attn_output.size(0), -1) # flattening output\n",
    "        out = self.fc1(attn_output)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "    def get_attention_weights(self):\n",
    "        return self.self_attention.in_proj_weight\n"
   ],
   "id": "779ea44aec42dd19",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:17:21.235713Z",
     "start_time": "2025-10-14T14:17:21.230510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = TweeterClassifier()\n",
    "criterion = torch.nn.BCELoss() # Binary Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ],
   "id": "bffcefca38941bc4",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:05:14.186014Z",
     "start_time": "2025-10-14T14:03:36.035721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(train_x), BATCH_SIZE):\n",
    "        batch_x = torch.tensor(train_x[i:i+BATCH_SIZE], dtype=torch.float32)\n",
    "        batch_y = torch.tensor(train_y[i:i+BATCH_SIZE].values, dtype=torch.float32).view(-1, 1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss/len(train_x)}')"
   ],
   "id": "9acdd12af9a5741",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.006537646255230623\n",
      "Epoch 2/5, Loss: 0.005141963395650173\n",
      "Epoch 3/5, Loss: 0.00453762426264793\n",
      "Epoch 4/5, Loss: 0.004121792647277579\n",
      "Epoch 5/5, Loss: 0.00380176213164317\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:14:07.277494Z",
     "start_time": "2025-10-14T14:14:05.491634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy = 0\n",
    "model.eval()\n",
    "with torch.no_grad(): # Disable gradient calculation for evaluation\n",
    "    for i in range(0, len(test_x), BATCH_SIZE):\n",
    "        batch_x = torch.tensor(test_x[i:i+BATCH_SIZE], dtype=torch.float32)\n",
    "        batch_y = test_y[i:i+BATCH_SIZE].values\n",
    "        outputs = model(batch_x)\n",
    "        predicted = (outputs.numpy() > 0.5).astype(int)\n",
    "        accuracy += sum((predicted.flatten() == batch_y))\n",
    "accuracy = accuracy / len(test_y)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ],
   "id": "480cb500a78fb81b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.15%\n"
     ]
    }
   ],
   "execution_count": 68
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
