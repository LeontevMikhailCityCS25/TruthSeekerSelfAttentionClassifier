{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Dataset origin:** https://www.unb.ca/cic/datasets/truthseeker-2023.html\n",
    "\n",
    "*S. Dadkhah, X. Zhang, A. G. Weismann, A. Firouzi and A. A. Ghorbani, \"The Largest Social Media Ground-Truth Dataset for Real/Fake Content: TruthSeeker,\" in IEEE Transactions on Computational Social Systems, 99. 1-15, Oct. 2023.*"
   ],
   "id": "c5dd158941a70349"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-15T14:03:04.823311Z",
     "start_time": "2025-10-15T14:03:02.859531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from embedder import Word2VecEmbedder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ],
   "id": "6ba7a6e85a5634e3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailleontev/PycharmProjects/Attestation/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:03:06.572858Z",
     "start_time": "2025-10-15T14:03:06.208464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PATH_TO_FILE =\"/Users/mikhailleontev/PycharmProjects/Attestation/TruthSeeker2023/Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(PATH_TO_FILE)\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "id": "4badfbb6737ecf6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134198, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0      author                                          statement  \\\n",
       "0           0  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "1           1  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "2           2  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "3           3  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "4           4  D.L. Davis  End of eviction moratorium means millions of A...   \n",
       "\n",
       "   target  BinaryNumTarget                 manual_keywords  \\\n",
       "0    True              1.0  Americans, eviction moratorium   \n",
       "1    True              1.0  Americans, eviction moratorium   \n",
       "2    True              1.0  Americans, eviction moratorium   \n",
       "3    True              1.0  Americans, eviction moratorium   \n",
       "4    True              1.0  Americans, eviction moratorium   \n",
       "\n",
       "                                               tweet 5_label_majority_answer  \\\n",
       "0  @POTUS Biden Blunders - 6 Month Update\\n\\nInfl...            Mostly Agree   \n",
       "1  @S0SickRick @Stairmaster_ @6d6f636869 Not as m...             NO MAJORITY   \n",
       "2  THE SUPREME COURT is siding with super rich pr...                   Agree   \n",
       "3  @POTUS Biden Blunders\\n\\nBroken campaign promi...            Mostly Agree   \n",
       "4  @OhComfy I agree. The confluence of events rig...                   Agree   \n",
       "\n",
       "  3_label_majority_answer  \n",
       "0                   Agree  \n",
       "1                   Agree  \n",
       "2                   Agree  \n",
       "3                   Agree  \n",
       "4                   Agree  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>statement</th>\n",
       "      <th>target</th>\n",
       "      <th>BinaryNumTarget</th>\n",
       "      <th>manual_keywords</th>\n",
       "      <th>tweet</th>\n",
       "      <th>5_label_majority_answer</th>\n",
       "      <th>3_label_majority_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@POTUS Biden Blunders - 6 Month Update\\n\\nInfl...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@S0SickRick @Stairmaster_ @6d6f636869 Not as m...</td>\n",
       "      <td>NO MAJORITY</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>THE SUPREME COURT is siding with super rich pr...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@POTUS Biden Blunders\\n\\nBroken campaign promi...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D.L. Davis</td>\n",
       "      <td>End of eviction moratorium means millions of A...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans, eviction moratorium</td>\n",
       "      <td>@OhComfy I agree. The confluence of events rig...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preparation of statements (news related to tweets)",
   "id": "da726fa4990c4e1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:03:08.194127Z",
     "start_time": "2025-10-15T14:03:08.167509Z"
    }
   },
   "cell_type": "code",
   "source": "df['statement'].unique()[:10]",
   "id": "45ad41f64e343b35",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.',\n",
       "       'The Trump administration worked to free 5,000 Taliban prisoners.',\n",
       "       'In Afghanistan, over 100 billion dollars spent on military contracts.',\n",
       "       'A photo shows two COVID-19 patients lying on the floor awaiting treatment in Florida.',\n",
       "       'Its been over 50 years since minimum (wage) and inflation parted ways, then over a decade since the federal minimum went up at all.',\n",
       "       'We have a record 9.3 million job openings in the U.S.',\n",
       "       'Since 1978, CEO compensation rose over 1,000% and only 11.9% for average workers.',\n",
       "       'Wisconsins 2019-21 budget produced the first positive general fund balance since 2000, and the governors proposed 2021-23 budget would return it to a deficit.',\n",
       "       'Opposition to having a fully elected Chicago Board of Education is in the super minority.',\n",
       "       'We now have more job openings than we do people who are on unemployment. We have 60% more job openings today than we did the month before the pandemic hit the state of Texas.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:03:19.105267Z",
     "start_time": "2025-10-15T14:03:09.377188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "statements= df['statement'].copy()\n",
    "codes = {}\n",
    "i = 0\n",
    "for j in statements.unique():\n",
    "    codes[i] = j\n",
    "    statements.replace(j, i, inplace=True)\n",
    "    i += 1\n",
    "print(codes[20])\n",
    "NUMBER_OF_CLASSES = len(codes)\n",
    "print(NUMBER_OF_CLASSES)"
   ],
   "id": "d405a5231605efec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president's own FBI director said that the greatest domestic terrorist threat is white supremacists.\n",
      "1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/5bbxd8_17jd09bs6yq9gc3c40000gn/T/ipykernel_15065/252864678.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  statements.replace(j, i, inplace=True)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T12:55:39.439411Z",
     "start_time": "2025-10-15T12:55:39.435856Z"
    }
   },
   "cell_type": "markdown",
   "source": "### Preparation of tweets",
   "id": "ed07638d63d05057"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:03:22.081707Z",
     "start_time": "2025-10-15T14:03:22.072375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tweets = df['tweet'].copy()\n",
    "tweets.head(10)"
   ],
   "id": "6ba5f6f1706dc98d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @POTUS Biden Blunders - 6 Month Update\\n\\nInfl...\n",
       "1    @S0SickRick @Stairmaster_ @6d6f636869 Not as m...\n",
       "2    THE SUPREME COURT is siding with super rich pr...\n",
       "3    @POTUS Biden Blunders\\n\\nBroken campaign promi...\n",
       "4    @OhComfy I agree. The confluence of events rig...\n",
       "5    I've said this before, but it really is incred...\n",
       "6    As many face backlogged rent payments, America...\n",
       "7    @Thomas1774Paine @JoeBiden\\n#DOJ@TheJusticeDep...\n",
       "8    @SocialismIsDone @TheeKHiveQueenB Its a win fo...\n",
       "9    @daysofarelives2 @Sen_JoeManchin There is not ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:04:44.075921Z",
     "start_time": "2025-10-15T14:04:24.324823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_tweets = []\n",
    "for i in range(len(tweets)):\n",
    "    tokens = nltk.word_tokenize(tweets.iloc[i])\n",
    "    tokenized_tweets.append(tokens)\n",
    "    if i % 10000 == 0:\n",
    "        print(f'Tokenized {i} tweets')\n",
    "print(tokenized_tweets[:5])"
   ],
   "id": "d34fd56a81b14296",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 0 tweets\n",
      "Tokenized 10000 tweets\n",
      "Tokenized 20000 tweets\n",
      "Tokenized 30000 tweets\n",
      "Tokenized 40000 tweets\n",
      "Tokenized 50000 tweets\n",
      "Tokenized 60000 tweets\n",
      "Tokenized 70000 tweets\n",
      "Tokenized 80000 tweets\n",
      "Tokenized 90000 tweets\n",
      "Tokenized 100000 tweets\n",
      "Tokenized 110000 tweets\n",
      "Tokenized 120000 tweets\n",
      "Tokenized 130000 tweets\n",
      "[['@', 'POTUS', 'Biden', 'Blunders', '-', '6', 'Month', 'Update', 'Inflation', ',', 'Delta', 'mismanagement', ',', 'COVID', 'for', 'kids', ',', 'Abandoning', 'Americans', 'in', 'Afghanistan', ',', 'Arming', 'the', 'Taliban', ',', 'S.', 'Border', 'crisis', ',', 'Breaking', 'job', 'growth', ',', 'Abuse', 'of', 'power', '(', 'Many', 'Exec', 'Orders', ',', '$', '3.5T', 'through', 'Reconciliation', ',', 'Eviction', 'Moratorium', ')', '...', 'what', 'did', 'I', 'miss', '?'], ['@', 'S0SickRick', '@', 'Stairmaster_', '@', '6d6f636869', 'Not', 'as', 'many', 'people', 'are', 'literally', 'starving', 'and', 'out', 'in', 'the', 'streets', 'as', 'they', 'were', 'in', 'the', '19th', 'century', '.', 'Isnt', 'capitalism', 'grand', '?', 'Meanwhile', ',', 'were', 'facing', 'an', 'eviction', 'moratorium', 'threatening', 'to', 'make', 'millions', 'of', 'Americans', 'homeless', '.', 'Fuck', 'off', 'with', 'this', 'corporatist', 'propaganda', '.'], ['THE', 'SUPREME', 'COURT', 'is', 'siding', 'with', 'super', 'rich', 'property', 'owners', 'and', 'over', 'poor', 'struggling', 'AMERICANS', 'by', 'blocking', 'the', 'eviction', 'moratorium', 'during', 'a', 'pandemic', 'which', 'is', \"n't\", 'even', 'over', 'yet', '.', 'ASSHOLES', '!', 'get', 'ready', 'for', 'more', 'homeless', 'people', '!'], ['@', 'POTUS', 'Biden', 'Blunders', 'Broken', 'campaign', 'promises', ',', 'Inflation', ',', 'Delta', 'mismanagement', ',', 'Dems', 'without', 'leadership', ',', 'Abandoned', 'Americans', '&', 'amp', ';', 'Armed', 'the', 'Taliban', ',', 'S.', 'Border', 'crisis', ',', 'Breaking', 'job', 'growth', ',', 'Abuse', 'of', 'power', '(', '61', 'Exec', 'Orders', ',', '$', '1.9T+', 'in', 'Reconciliation', ',', 'Eviction', 'Moratorium', ',', 'Vaccine', 'Mandates', ')'], ['@', 'OhComfy', 'I', 'agree', '.', 'The', 'confluence', 'of', 'events', 'right', 'now', 'is', 'unprecedented', '(', 'Afghan', 'disaster', ',', 'bombing', ',', 'overturning', 'of', 'the', 'eviction', 'moratorium', ',', 'collapse', 'of', 'Biden', 'admin', 'credibility', ')', '.', 'If', 'images', 'come', 'out', 'of', 'Americans', 'being', 'executed', 'by', 'Taliban', ',', 'SHTF', '.']]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:04:54.503030Z",
     "start_time": "2025-10-15T14:04:54.455890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tweet_lengths = [len(tokens) for tokens in tokenized_tweets]\n",
    "print(f'Mean length of tokenized tweets: {np.mean(tweet_lengths)}')\n",
    "print(f'Median tweet length: {np.median(tweet_lengths)}')\n",
    "print(f'Max length of tokenized tweets: {max(tweet_lengths)}')\n",
    "print(f'Min length of tokenized tweets: {min(tweet_lengths)}')\n"
   ],
   "id": "a0d0679bafa7ff16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of tokenized tweets: 42.12054576074159\n",
      "Median tweet length: 44.0\n",
      "Max length of tokenized tweets: 174\n",
      "Min length of tokenized tweets: 1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:04:56.734988Z",
     "start_time": "2025-10-15T14:04:56.501336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CAP_LENGTH = 50\n",
    "tweets_capped = [tokens[:CAP_LENGTH] for tokens in tokenized_tweets]\n",
    "print(tweets_capped[:3])"
   ],
   "id": "421989ade9d3c81e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['@', 'POTUS', 'Biden', 'Blunders', '-', '6', 'Month', 'Update', 'Inflation', ',', 'Delta', 'mismanagement', ',', 'COVID', 'for', 'kids', ',', 'Abandoning', 'Americans', 'in', 'Afghanistan', ',', 'Arming', 'the', 'Taliban', ',', 'S.', 'Border', 'crisis', ',', 'Breaking', 'job', 'growth', ',', 'Abuse', 'of', 'power', '(', 'Many', 'Exec', 'Orders', ',', '$', '3.5T', 'through', 'Reconciliation', ',', 'Eviction', 'Moratorium', ')'], ['@', 'S0SickRick', '@', 'Stairmaster_', '@', '6d6f636869', 'Not', 'as', 'many', 'people', 'are', 'literally', 'starving', 'and', 'out', 'in', 'the', 'streets', 'as', 'they', 'were', 'in', 'the', '19th', 'century', '.', 'Isnt', 'capitalism', 'grand', '?', 'Meanwhile', ',', 'were', 'facing', 'an', 'eviction', 'moratorium', 'threatening', 'to', 'make', 'millions', 'of', 'Americans', 'homeless', '.', 'Fuck', 'off', 'with', 'this', 'corporatist'], ['THE', 'SUPREME', 'COURT', 'is', 'siding', 'with', 'super', 'rich', 'property', 'owners', 'and', 'over', 'poor', 'struggling', 'AMERICANS', 'by', 'blocking', 'the', 'eviction', 'moratorium', 'during', 'a', 'pandemic', 'which', 'is', \"n't\", 'even', 'over', 'yet', '.', 'ASSHOLES', '!', 'get', 'ready', 'for', 'more', 'homeless', 'people', '!']]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:04:58.586902Z",
     "start_time": "2025-10-15T14:04:58.583591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Word2Vec Hyper parameters\n",
    "VECTOR_SIZE = 64\n",
    "WINDOW = 5\n",
    "WORKERS = 4"
   ],
   "id": "c5e11438405ee0a4",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:05:10.308750Z",
     "start_time": "2025-10-15T14:05:02.002477Z"
    }
   },
   "cell_type": "code",
   "source": "model_vec = Word2VecEmbedder(tweets_capped, vector_size=VECTOR_SIZE, window=WINDOW, min_count=1, workers=WORKERS)",
   "id": "4adf0c62276896f3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:05:32.136712Z",
     "start_time": "2025-10-15T14:05:32.132891Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(model_vec.embed(tweets_capped[0])))",
   "id": "221083500a1f716e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:05:47.379055Z",
     "start_time": "2025-10-15T14:05:40.010930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedded_tweets = []\n",
    "for tokens in tweets_capped:\n",
    "    tweet_vector = model_vec.embed(tokens)\n",
    "    # Pad with zero vectors if tweet is shorter than CAP_LENGTH\n",
    "    while len(tweet_vector) < CAP_LENGTH:\n",
    "        tweet_vector.append(np.zeros(VECTOR_SIZE))\n",
    "    embedded_tweets.append(tweet_vector)\n",
    "embedded_tweets = np.array(embedded_tweets) # turn into numpy array\n",
    "print(embedded_tweets.shape)"
   ],
   "id": "3c3d53359a849963",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134198, 50, 64)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:11:20.142028Z",
     "start_time": "2025-10-15T14:11:19.372297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TEST_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "SHUFFLE = True\n",
    "train_x, test_x, train_y, test_y = train_test_split(embedded_tweets, statements, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=SHUFFLE)\n",
    "print(f'Train shape: {train_x.shape}, Test shape: {test_x.shape}')\n",
    "print(f'Train labels shape: {train_y.shape}, Test labels shape: {test_y.shape}')"
   ],
   "id": "2e5d185aa4447143",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (120778, 50, 64), Test shape: (13420, 50, 64)\n",
      "Train labels shape: (120778,), Test labels shape: (13420,)\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define torch model",
   "id": "d8b990bff8a3adb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:38:50.257082Z",
     "start_time": "2025-10-15T14:38:50.250437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define torch model\n",
    "# Hyperparameters\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUMBER_OF_HEADS = 4\n",
    "DROPOUT_RATE = 0.3\n",
    "class TweeterClassifierStatements(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TweeterClassifierStatements, self).__init__()\n",
    "        self.self_attention = torch.nn.MultiheadAttention(embed_dim=VECTOR_SIZE, num_heads=NUMBER_OF_HEADS)\n",
    "        self.fc1 = torch.nn.Linear(VECTOR_SIZE * CAP_LENGTH, 256)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(DROPOUT_RATE)\n",
    "        self.fc2 = torch.nn.Linear(256, NUMBER_OF_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)  # Change shape to (seq_len, batch_size, embed_dim) for MultiheadAttention\n",
    "        attn_output, _ = self.self_attention(x, x, x)\n",
    "        attn_output = attn_output.permute(1, 0, 2)  # Back to (batch_size, seq_len, embed_dim)\n",
    "        attn_output = attn_output.reshape(attn_output.size(0), -1)  # Flattening output\n",
    "        x = self.fc1(attn_output)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "id": "26757eea1822ff40",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:38:51.680618Z",
     "start_time": "2025-10-15T14:38:51.670563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = TweeterClassifierStatements()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ],
   "id": "c7f60c8b60d0952",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T14:50:12.248683Z",
     "start_time": "2025-10-15T14:38:52.991901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    permutation = np.random.permutation(train_x.shape[0])\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, train_x.shape[0], BATCH_SIZE):\n",
    "        indices = permutation[i:i+BATCH_SIZE]\n",
    "        batch_x, batch_y = train_x[indices], train_y.iloc[indices]\n",
    "        batch_x_tensor = torch.tensor(batch_x, dtype=torch.float32)\n",
    "        batch_y_tensor = torch.tensor(batch_y.values, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x_tensor)\n",
    "        loss = criterion(outputs, batch_y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss}')"
   ],
   "id": "59584ef891982609",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 8611.50347456336\n",
      "Epoch 2/25, Loss: 4244.128017768264\n",
      "Epoch 3/25, Loss: 3265.8554413989186\n",
      "Epoch 4/25, Loss: 2807.796820972115\n",
      "Epoch 5/25, Loss: 2459.625379288569\n",
      "Epoch 6/25, Loss: 2204.647118275985\n",
      "Epoch 7/25, Loss: 2092.054299581796\n",
      "Epoch 8/25, Loss: 2003.322446545586\n",
      "Epoch 9/25, Loss: 1905.2302094791085\n",
      "Epoch 10/25, Loss: 1874.3509437348694\n",
      "Epoch 11/25, Loss: 1876.5727614364587\n",
      "Epoch 12/25, Loss: 1804.8124699557666\n",
      "Epoch 13/25, Loss: 1795.346362634562\n",
      "Epoch 14/25, Loss: 1853.4327622661367\n",
      "Epoch 15/25, Loss: 1838.3827989145648\n",
      "Epoch 16/25, Loss: 1838.0893814766896\n",
      "Epoch 17/25, Loss: 1824.9691503612266\n",
      "Epoch 18/25, Loss: 1881.1593841760186\n",
      "Epoch 19/25, Loss: 1863.1354634733289\n",
      "Epoch 20/25, Loss: 1885.4942440406303\n",
      "Epoch 21/25, Loss: 1920.3143676241161\n",
      "Epoch 22/25, Loss: 1870.282447234611\n",
      "Epoch 23/25, Loss: 1917.7542819452065\n",
      "Epoch 24/25, Loss: 1953.6791880533565\n",
      "Epoch 25/25, Loss: 1937.7106809751567\n"
     ]
    }
   ],
   "execution_count": 71
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
